Aside from the agent and the environment, a reinforcement learning model has four essential components: a policy, a reward, a value function, and an environment model.

1. A policy determines how an agent behaves at a specific point in time. In 
   broad terms, it is a mapping between environmental conditions to actions, to the activities that the agent conducts in the environment. The policy might be as basic as a function or as sophisticated as function calculations. The policy is at the heart of everything the agent discovers.
2. A reward defines the goal of an RL issue. At each time step, the agent’s
   behaviors result in a reward. The ultimate purpose of the agent is to optimize the overall reward earned. As a consequence, the reward differentiates between positive and bad action outcomes for the agent. In a natural system, we may attribute rewards and punishments as delightful and unpleasant experiences.
3. 
   A state’s value is the total accumulated quantity of prizes that the agent may anticipate receiving in the future if it begins in that condition. Values represent the long-term attractiveness of a collection of states based on the expected future states and the benefits produced by those states. Even though a state produces a modest immediate reward, it might still be valuable since it is frequently followed by additional states that produce bigger benefits.

4. 
   The environment model is another significant component of several reinforcement learning systems. This is a mechanism that mimics environmental behavior and enables predictions about how the environment will respond. This model will let the agent forecast the next reward if an action is taken, enabling the agent to rely on developing action on future environmental responses.
   
   ![Alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/250px-Reinforcement_learning_diagram.svg.png)